The purpose of this project is to accurately calibrate a 3D camera with respect to a conventional camera, by finding the relationship between them. Before going into details, I will present some facts about Microsoft's 3D camera, the Kinect. \\

\section{About the Kinect}

The Kinect is a motion sensing device, initially built as an add-on for the Microsoft XBOX 360 gaming console, but later on ported to the PC. It is a relatively inexpensive device that incorporates a lot of different sensors like an RGB camera, a depth sensor and a multi-array microphone which provide full-body 3D motion capture [Wikipedia]. The depth sensor is a combination of an infrared laser and a monochrome CMOS sensor which can capture 3D data under any lighting conditions. \\
The sensors being used for this project are:

\begin{itemize}
	\item RGB camera
	\item Depth sensor: monochrome CMOS and infrared laser
\end{itemize}

\noindent
The RGB camera can capture 8-bit images at a VGA resolution (640 x 480 pixels). However, the hardware is capable of capturing images at higher resolutions (1280 x 1024 pixels), but at a lower frame rate. The frame rate for video capture from the RGB camera can vary between 9 Hz and 30 Hz.
\\
The second sensor used, the depth sensor, is very important for the 3D sensing capabilities of the device. The monochrome camera can capture images at VGA resolution (640 x 480 pixels) with 11-bit depth, which can assure 2,048 levels of sensitivity. The sensor has a practical range between {\bf 1.2 meters} and {\bf 3.5 meters}. In order to be able to take advantage of the Kinect's motion sensing abilities an area of 6 $m^2$ is needed. 
\\
The Kinect connects to the PC or XBOX using a USB interface, but it requires an additional power supply due to the motorized tilt mechanism. 
\\
\noindent
For those who are interested to develop applications for the Kinect, the latest SDK (at this date) is Microsoft Kinect SDK 1.7 and it can be found at the address below:

\url{http://www.microsoft.com/en-us/kinectforwindows/develop/developer-downloads.aspx}

\noindent
Since the Kinect has been received with a lot of enthusiasm by the developers, there are some 3rd party libraries and drivers that can be used. Some of the most famous ones are:
\begin{itemize}
	\item OpenNI \url{http://www.openni.org/}
	\item libfreenect, part of the Open Kinect project \url{http://openkinect.org/wiki/Main_Page}
\end{itemize}

\section{Depth camera calibration}
\noindent
The depth camera calibration is a process in which we try to reproject a 3D point to a 2D point captured by the normal RGB camera. Without a calibration this would not be possible. The calibration helps to accurately detect what 2D point from the RGB image corresponds to what 3D point (in our case detected by the Kinect).
\\\\
So, how can we project a 3D point to a 2D point?
\\\\
\[	A =
            \left[ {\begin{array}{cccc}
             a_{11} & a_{12} & a_{13} & a_{14} \\
             a_{21} & a_{22} & a_{23} & a_{24} \\
             a_{31} & a_{32} & a_{33} & a_{34} \\             
             \end{array} } \right]
\]

\noindent
A represents the calibration matrix. The purpose of any calibration is to identify this matrix accurately. Using this matrix we can project a 3D point $(x,y,z)$ to a 2D point $(u,v)$. 
\\\\
Based on the calibration matrix, the projection point is computed in the following way:

$$ u = \frac{a_{11} \cdot x + a_{12} \cdot y + a_{13} \cdot z + a_{14}}{a_{31} \cdot x + a_{32} \cdot y + a_{33} \cdot z + a_{34}} $$

$$ v = \frac{a_{21} \cdot x + a_{22} \cdot y + a_{23} \cdot z + a_{24}}{a_{31} \cdot x + a_{32} \cdot y + a_{33} \cdot z + a_{34}} $$

\noindent
This formula will be later used for the assessment of the result.

\section{Motivation of this project}
\noindent
The process in which a 3D point is reprojected to a 2D image is not new. Depth camera calibration has been done before, but this process implies to manually find a minimum number of correspondences. Two points are valid points for the set of correspondences if the points in the 2D image and the 3D image represent the same physical point (pixel in the image plane). Once the points have been found, we try to estimate the calibration matrix using a model. Based on this model all the other points will be reprojected and thus the calibration is completed.  
\\
\noindent
Finding the correspondences between the 2D image and the 3D image is a tedious work and it implies a lot of time. This is why an automatic depth camera calibration would be useful. By having an automatic way to detect the correspondences, the user only needs to walk with a patter (e.g a cone) in front of the camera and the "good" points will be selected automatically. What exactly are good points? We will describe this in the following chapters. 
\\\\
\noindent
This document will describe in the following chapters a way in which the automatic depth camera calibration can be achieved. The design chapter describes the process from the algorithm point of view, what methods we have used and why. The implementation chapter will go into more implementation details and will describe the functions that make the algorithm work. In order to be able to asses our results, we need some visualization tools. Chapter 4 will describe the visual interfaces and the final result, the calibration.    
